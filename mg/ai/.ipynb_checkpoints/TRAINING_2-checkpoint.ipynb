{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab146f5",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1fedc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from fractions import Fraction\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53ed01",
   "metadata": {},
   "source": [
    "### THE BELOW CELL IS THE ONLY CELL WE NEED TO CHANGE TO CREATE NEW AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24926757",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = \"Class\" # Class, Jazz, Pop, etc.\n",
    "key = \"C\" # C, C#, D, D#, E, F, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3944134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"train_music/\" + genre + key + \"/\"\n",
    "model_name = genre + key + \".h5\"\n",
    "\n",
    "unique_x_file = genre + key + \"_unique.txt\"\n",
    "xval_file = genre + key + \"_x.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be387ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append((str(element.pitch), element.quarterLength))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append(('.'.join(str(n) for n in element.normalOrder), element.quarterLength))\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7ba5a",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a447c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading notes...\n",
      "Loading Music File: train_music/ClassC/2721imstpet1.mid\n",
      "Loading Music File: train_music/ClassC/beethoven_sonata_21_1st_mvt_PNO.mid\n",
      "Loading Music File: train_music/ClassC/chpn_op10_e01.mid\n",
      "Loading Music File: train_music/ClassC/kv545-allegro.mid\n",
      "Loading Music File: train_music/ClassC/massenet_meditationPNO.mid\n",
      "Loading Music File: train_music/ClassC/rachmm6.mid\n",
      "Loading Music File: train_music/ClassC/schubert_wanderer_760_1_(c)yogore.mid\n",
      "Loading Music File: train_music/ClassC/schubert_wanderer_760_2_(c)yogore.mid\n",
      "Loading Music File: train_music/ClassC/schubert_wanderer_760_3_(c)yogore.mid\n",
      "Loading Music File: train_music/ClassC/sonata_10_1_(c)iscenko.mid\n",
      "Loading Music File: train_music/ClassC/sonata_11_2_(c)iscenko.mid\n"
     ]
    }
   ],
   "source": [
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "print(\"Loading notes...\")\n",
    "notes_array = np.asarray([read_midi(path+i) for i in files], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c161548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# every note in a 1d array\n",
    "notes_ = [element for note_ in notes_array for element in note_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a1b2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of each note\n",
    "freq = dict(Counter(notes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509395bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of frequent notes: 189\n"
     ]
    }
   ],
   "source": [
    "# list of frequent notes, adjustable by count\n",
    "frequent_notes = [note_ for note_, count in freq.items() if count>=30]\n",
    "print(\"# of frequent notes:\", len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d95edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_music = notes - nonfrequent notes\n",
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.asarray(new_music, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c7a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:\t (14720, 32, 2)\n",
      "output shape:\t (14720, 2)\n"
     ]
    }
   ],
   "source": [
    "# prepare input and output sequences\n",
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output_ = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output_)\n",
    "        \n",
    "\n",
    "print(\"input shape:\\t\", np.asarray(x).shape)\n",
    "print(\"output shape:\\t\", np.asarray(y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae77a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique note --> integer\n",
    "unique_x=[]\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(x[i])):\n",
    "        unique_x.append(x[i][j])\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
    "\n",
    "with open(unique_x_file, \"w\") as output:\n",
    "    for element in unique_x:\n",
    "        output.write(str(element)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1738616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_seq <--> model input\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34b952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same procedure as x_seq but simpler\n",
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq = np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65fe7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test partitioning, test_size=0.2 --> 20% of data is test\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)\n",
    "np.savetxt(xval_file, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae66047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "x_tr: (11776, 32)\n",
      "y_tr: (11776,)\n",
      "x_val: (2944, 32)\n",
      "y_val: (2944,)\n"
     ]
    }
   ],
   "source": [
    "# metadata\n",
    "print(\"Shapes:\")\n",
    "print(\"x_tr:\", x_tr.shape)\n",
    "print(\"y_tr:\", y_tr.shape)\n",
    "print(\"x_val:\", x_val.shape)\n",
    "print(\"y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92180e6e",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f568b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 100)           47104000  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 32, 64)            19264     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 16, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 16, 128)           24704     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 8, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 8, 256)            98560     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 256)            0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 4, 256)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 189)               48573     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,360,893\n",
      "Trainable params: 47,360,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# NN design\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "    \n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "165f9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3dfc7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 4.8708\n",
      "Epoch 1: val_loss improved from inf to 4.73705, saving model to ClassC.h5\n",
      "92/92 [==============================] - 62s 657ms/step - loss: 4.8708 - val_loss: 4.7371\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 4.5233\n",
      "Epoch 2: val_loss improved from 4.73705 to 4.40801, saving model to ClassC.h5\n",
      "92/92 [==============================] - 61s 659ms/step - loss: 4.5233 - val_loss: 4.4080\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 4.1443\n",
      "Epoch 3: val_loss improved from 4.40801 to 4.15374, saving model to ClassC.h5\n",
      "92/92 [==============================] - 57s 620ms/step - loss: 4.1443 - val_loss: 4.1537\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 3.8759\n",
      "Epoch 4: val_loss improved from 4.15374 to 3.97492, saving model to ClassC.h5\n",
      "92/92 [==============================] - 60s 655ms/step - loss: 3.8759 - val_loss: 3.9749\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 3.6942\n",
      "Epoch 5: val_loss improved from 3.97492 to 3.84051, saving model to ClassC.h5\n",
      "92/92 [==============================] - 58s 632ms/step - loss: 3.6942 - val_loss: 3.8405\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 3.5491\n",
      "Epoch 6: val_loss improved from 3.84051 to 3.72112, saving model to ClassC.h5\n",
      "92/92 [==============================] - 62s 674ms/step - loss: 3.5491 - val_loss: 3.7211\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 3.4225\n",
      "Epoch 7: val_loss improved from 3.72112 to 3.65614, saving model to ClassC.h5\n",
      "92/92 [==============================] - 58s 634ms/step - loss: 3.4225 - val_loss: 3.6561\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 3.3114\n",
      "Epoch 8: val_loss improved from 3.65614 to 3.55967, saving model to ClassC.h5\n",
      "92/92 [==============================] - 60s 651ms/step - loss: 3.3114 - val_loss: 3.5597\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 3.2039\n",
      "Epoch 9: val_loss improved from 3.55967 to 3.48955, saving model to ClassC.h5\n",
      "92/92 [==============================] - 57s 625ms/step - loss: 3.2039 - val_loss: 3.4895\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 3.1125\n",
      "Epoch 10: val_loss improved from 3.48955 to 3.45187, saving model to ClassC.h5\n",
      "92/92 [==============================] - 65s 704ms/step - loss: 3.1125 - val_loss: 3.4519\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 3.0394\n",
      "Epoch 11: val_loss improved from 3.45187 to 3.39324, saving model to ClassC.h5\n",
      "92/92 [==============================] - 62s 671ms/step - loss: 3.0394 - val_loss: 3.3932\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.9739\n",
      "Epoch 12: val_loss improved from 3.39324 to 3.35778, saving model to ClassC.h5\n",
      "92/92 [==============================] - 62s 669ms/step - loss: 2.9739 - val_loss: 3.3578\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.9094\n",
      "Epoch 13: val_loss improved from 3.35778 to 3.33391, saving model to ClassC.h5\n",
      "92/92 [==============================] - 57s 624ms/step - loss: 2.9094 - val_loss: 3.3339\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.8450\n",
      "Epoch 14: val_loss improved from 3.33391 to 3.28247, saving model to ClassC.h5\n",
      "92/92 [==============================] - 61s 660ms/step - loss: 2.8450 - val_loss: 3.2825\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.7866\n",
      "Epoch 15: val_loss improved from 3.28247 to 3.23756, saving model to ClassC.h5\n",
      "92/92 [==============================] - 59s 639ms/step - loss: 2.7866 - val_loss: 3.2376\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.7289\n",
      "Epoch 16: val_loss improved from 3.23756 to 3.21259, saving model to ClassC.h5\n",
      "92/92 [==============================] - 60s 653ms/step - loss: 2.7289 - val_loss: 3.2126\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.6809\n",
      "Epoch 17: val_loss improved from 3.21259 to 3.19558, saving model to ClassC.h5\n",
      "92/92 [==============================] - 59s 637ms/step - loss: 2.6809 - val_loss: 3.1956\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.6248\n",
      "Epoch 18: val_loss improved from 3.19558 to 3.19035, saving model to ClassC.h5\n",
      "92/92 [==============================] - 60s 655ms/step - loss: 2.6248 - val_loss: 3.1904\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.5766\n",
      "Epoch 19: val_loss did not improve from 3.19035\n",
      "92/92 [==============================] - 55s 595ms/step - loss: 2.5766 - val_loss: 3.1920\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.5310\n",
      "Epoch 20: val_loss improved from 3.19035 to 3.14747, saving model to ClassC.h5\n",
      "92/92 [==============================] - 63s 685ms/step - loss: 2.5310 - val_loss: 3.1475\n",
      "Epoch 21/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.4961\n",
      "Epoch 21: val_loss improved from 3.14747 to 3.13980, saving model to ClassC.h5\n",
      "92/92 [==============================] - 63s 685ms/step - loss: 2.4961 - val_loss: 3.1398\n",
      "Epoch 22/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.4520\n",
      "Epoch 22: val_loss improved from 3.13980 to 3.11908, saving model to ClassC.h5\n",
      "92/92 [==============================] - 58s 632ms/step - loss: 2.4520 - val_loss: 3.1191\n",
      "Epoch 23/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.4075\n",
      "Epoch 23: val_loss improved from 3.11908 to 3.09780, saving model to ClassC.h5\n",
      "92/92 [==============================] - 59s 646ms/step - loss: 2.4075 - val_loss: 3.0978\n",
      "Epoch 24/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.3686\n",
      "Epoch 24: val_loss did not improve from 3.09780\n",
      "92/92 [==============================] - 57s 616ms/step - loss: 2.3686 - val_loss: 3.0982\n",
      "Epoch 25/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.3467\n",
      "Epoch 25: val_loss improved from 3.09780 to 3.08920, saving model to ClassC.h5\n",
      "92/92 [==============================] - 65s 710ms/step - loss: 2.3467 - val_loss: 3.0892\n",
      "Epoch 26/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.3065\n",
      "Epoch 26: val_loss improved from 3.08920 to 3.05869, saving model to ClassC.h5\n",
      "92/92 [==============================] - 63s 680ms/step - loss: 2.3065 - val_loss: 3.0587\n",
      "Epoch 27/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.2616\n",
      "Epoch 27: val_loss did not improve from 3.05869\n",
      "92/92 [==============================] - 55s 599ms/step - loss: 2.2616 - val_loss: 3.0699\n",
      "Epoch 28/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.2321\n",
      "Epoch 28: val_loss improved from 3.05869 to 3.03840, saving model to ClassC.h5\n",
      "92/92 [==============================] - 59s 646ms/step - loss: 2.2321 - val_loss: 3.0384\n",
      "Epoch 29/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.2018\n",
      "Epoch 29: val_loss improved from 3.03840 to 3.03521, saving model to ClassC.h5\n",
      "92/92 [==============================] - 62s 671ms/step - loss: 2.2018 - val_loss: 3.0352\n",
      "Epoch 30/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.1662\n",
      "Epoch 30: val_loss improved from 3.03521 to 3.02144, saving model to ClassC.h5\n",
      "92/92 [==============================] - 62s 674ms/step - loss: 2.1662 - val_loss: 3.0214\n",
      "Epoch 31/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.1366\n",
      "Epoch 31: val_loss did not improve from 3.02144\n",
      "92/92 [==============================] - 59s 644ms/step - loss: 2.1366 - val_loss: 3.0222\n",
      "Epoch 32/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.0992\n",
      "Epoch 32: val_loss did not improve from 3.02144\n",
      "92/92 [==============================] - 58s 632ms/step - loss: 2.0992 - val_loss: 3.0218\n",
      "Epoch 33/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.0671\n",
      "Epoch 33: val_loss improved from 3.02144 to 3.00024, saving model to ClassC.h5\n",
      "92/92 [==============================] - 59s 640ms/step - loss: 2.0671 - val_loss: 3.0002\n",
      "Epoch 34/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 2.0384\n",
      "Epoch 34: val_loss did not improve from 3.00024\n",
      "92/92 [==============================] - 56s 605ms/step - loss: 2.0384 - val_loss: 3.0174\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - ETA: 0s - loss: 2.0175\n",
      "Epoch 35: val_loss improved from 3.00024 to 2.99209, saving model to ClassC.h5\n",
      "92/92 [==============================] - 60s 655ms/step - loss: 2.0175 - val_loss: 2.9921\n",
      "Epoch 36/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.9773\n",
      "Epoch 36: val_loss improved from 2.99209 to 2.98197, saving model to ClassC.h5\n",
      "92/92 [==============================] - 59s 638ms/step - loss: 1.9773 - val_loss: 2.9820\n",
      "Epoch 37/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.9603\n",
      "Epoch 37: val_loss did not improve from 2.98197\n",
      "92/92 [==============================] - 55s 595ms/step - loss: 1.9603 - val_loss: 3.0148\n",
      "Epoch 38/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.9414\n",
      "Epoch 38: val_loss did not improve from 2.98197\n",
      "92/92 [==============================] - 55s 600ms/step - loss: 1.9414 - val_loss: 2.9832\n",
      "Epoch 39/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.9143\n",
      "Epoch 39: val_loss improved from 2.98197 to 2.97864, saving model to ClassC.h5\n",
      "92/92 [==============================] - 68s 743ms/step - loss: 1.9143 - val_loss: 2.9786\n",
      "Epoch 40/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.8971\n",
      "Epoch 40: val_loss did not improve from 2.97864\n",
      "92/92 [==============================] - 65s 705ms/step - loss: 1.8971 - val_loss: 2.9857\n",
      "Epoch 41/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.8617\n",
      "Epoch 41: val_loss did not improve from 2.97864\n",
      "92/92 [==============================] - 65s 707ms/step - loss: 1.8617 - val_loss: 2.9937\n",
      "Epoch 42/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.8570\n",
      "Epoch 42: val_loss did not improve from 2.97864\n",
      "92/92 [==============================] - 65s 708ms/step - loss: 1.8570 - val_loss: 3.0086\n",
      "Epoch 43/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.8346\n",
      "Epoch 43: val_loss did not improve from 2.97864\n",
      "92/92 [==============================] - 65s 710ms/step - loss: 1.8346 - val_loss: 3.0033\n",
      "Epoch 44/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.8061\n",
      "Epoch 44: val_loss improved from 2.97864 to 2.97494, saving model to ClassC.h5\n",
      "92/92 [==============================] - 71s 770ms/step - loss: 1.8061 - val_loss: 2.9749\n",
      "Epoch 45/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.7684\n",
      "Epoch 45: val_loss did not improve from 2.97494\n",
      "92/92 [==============================] - 66s 712ms/step - loss: 1.7684 - val_loss: 3.0056\n",
      "Epoch 46/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.7643\n",
      "Epoch 46: val_loss did not improve from 2.97494\n",
      "92/92 [==============================] - 65s 706ms/step - loss: 1.7643 - val_loss: 3.0010\n",
      "Epoch 47/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.7166\n",
      "Epoch 47: val_loss did not improve from 2.97494\n",
      "92/92 [==============================] - 65s 704ms/step - loss: 1.7166 - val_loss: 3.0163\n",
      "Epoch 48/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.7310\n",
      "Epoch 48: val_loss did not improve from 2.97494\n",
      "92/92 [==============================] - 64s 694ms/step - loss: 1.7310 - val_loss: 2.9937\n",
      "Epoch 49/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.7078\n",
      "Epoch 49: val_loss did not improve from 2.97494\n",
      "92/92 [==============================] - 65s 702ms/step - loss: 1.7078 - val_loss: 3.0025\n",
      "Epoch 50/50\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.6940\n",
      "Epoch 50: val_loss did not improve from 2.97494\n",
      "92/92 [==============================] - 67s 734ms/step - loss: 1.6940 - val_loss: 3.0080\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "6efa788cb94cb17b6d18a0713a162526dde847b9fce190a5e1ec27ce6e1c4702"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
