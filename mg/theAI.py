from keras.models import load_model
import random
import numpy as np
from fractions import Fraction
from music21 import *

#print("SANITY CHECK")

def generateMusic(inst, artist, user_tempo, measures, music_file, showSheetMusic=False):
	
	# file references
	path = "train_music/" + inst + artist + "/"
	model_name = inst + artist + ".h5"

	unique_x_file = inst + artist + "_unique.txt"
	xval_file = inst + artist + "_x.txt"

	# USER-DEFINED TEMPO
	if user_tempo == "Largo":
		bpm = 50
	elif user_tempo == "Adagio":
		bpm = 70
	elif user_tempo == "Andante":
		bpm = 80
	elif user_tempo == "Moderato":
		bpm = 110
	elif user_tempo == "Allegro":
		bpm = 140
	elif user_tempo == "Presto":
		bpm = 180
	elif user_tempo == "Slow":
		bpm = 60
	elif user_tempo == "Moderate":
		bpm = 100
	elif user_tempo == "Fast":
		bpm = 150
	else:
		print("Tempo \'" + user_tempo + "\' is invalid.")
		print("Tempo automatically set to 60 bpm.")
		bpm = 60
	bpm = tempo.MetronomeMark(bpm)

	# programmer-defined timesteps (shouldn't mess with, at least for now)
	no_of_timesteps = 32

	# Music Generation
	model = load_model('ai/'+model_name)
	x_val = np.genfromtxt('ai/'+xval_file)
	with open('ai/'+unique_x_file, "r") as file:
		unique_x = file.read().splitlines()
	unique_x = [eval(line) for line in unique_x]

	ind = np.random.randint(0,len(x_val)-1)

	random_music = x_val[ind]

	predictions=[]
	count = 0
	measure_lim = 4 * measures
	while count <= measure_lim:

		random_music = random_music.reshape(1,no_of_timesteps)

		prob  = model.predict(random_music, verbose=0)[0] # MODEL PREDICTION
		y_pred = np.argmax(prob,axis=0)
		predictions.append(y_pred)

		random_music = np.insert(random_music[0],len(random_music[0]),y_pred)
		random_music = random_music[1:]

		x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x))
		predicted_notes = [x_int_to_note[i] for i in predictions]

		count += predicted_notes[len(predicted_notes) - 1][1]

	def convert_to_midi(prediction_output, inst):
		"""
	    Converts prediction_output (sequence of notes) to MIDI file

	    Parameters
	    ----------
	    prediction_output : 1d array of notes encoded as integers

	    Returns
	    -------
	    none

	    """
	   
		offset = 0
		output_notes = []

		myInstrument = ''
		if inst == 'Piano':
			myInstrument = instrument.Piano()
		elif inst == 'Violin':
			myInstrument = instrument.Violin()
		else:
			myInstrument = instrument.Flute()
		#myInstrument = instrument.Shamisen()

	    # create note and chord objects based on the values generated by the model
		for pattern in prediction_output:
	        
	        # pattern is a chord
			if ('.' in pattern[0]) or pattern[0].isdigit():
				notes_in_chord = pattern[0].split('.')
				notes = []
				for current_note in notes_in_chord: 
					cn=int(current_note)
					new_note = note.Note(cn)
					new_note.storedInstrument = myInstrument
					new_note.quarterLength = pattern[1]
					notes.append(new_note)

				new_chord = chord.Chord(notes)
				new_chord.offset = offset
				output_notes.append(new_chord)

			elif 'rest' in pattern[0]:
				new_rest = note.Rest(quarterLength=pattern[1])
				output_notes.append(new_rest)
	            
	        # pattern is a note
			else:
				new_note = note.Note(pattern[0])
				new_note.offset = offset
				new_note.storedInstrument = myInstrument
				new_note.quarterLength = pattern[1]
				output_notes.append(new_note)

	        # increase offset each iteration so that notes do not stack
			offset += 1
		midi_stream = stream.Stream()
		midi_stream.insert(myInstrument)
		midi_stream.append(bpm)
		midi_stream.append(output_notes)
		midi_stream.write('midi', fp=music_file)
		midi_stream.show('midi')
		if showSheetMusic:
			s = stream.Score()
			s.insert(0, metadata.Metadata())
			s.metadata.title = artist + " AI Music" 
			s.append(midi_stream)
			s.show('xml')

	convert_to_midi(predicted_notes, inst)
	print("\n")
	print("Generated music saved in", music_file)
